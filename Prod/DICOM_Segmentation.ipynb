{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage import exposure\n",
    "from skimage.morphology import dilation, square\n",
    "from skimage.restoration import denoise_tv_chambolle\n",
    "from skimage.measure import regionprops\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "from pydicom import dcmread\n",
    "from pydicom.encaps import encapsulate\n",
    "from pydicom.pixel_data_handlers.util import apply_modality_lut\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "from utils.function_lib import *\n",
    "\n",
    "from FusionSystem.FusionNetwork import FusionNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "transform_ct = transforms.Compose(\n",
    "    [transforms.ToPILImage(),\n",
    "     transforms.Resize(256, interpolation=transforms.functional.InterpolationMode.BILINEAR, antialias=True),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.2], std=[0.2])\n",
    "     ])\n",
    "\n",
    "transform_liver = transforms.Compose(\n",
    "    [transforms.ToPILImage(),\n",
    "     transforms.Resize(256, interpolation=transforms.functional.InterpolationMode.BILINEAR, antialias=True),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "     ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Incarcam modelul"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading models\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "model = FusionNetwork('G:/MachineLearning/models/', threshold=0.5)\n",
    "model_lesion = FusionNetwork('G:/MachineLearning/models-lesion/', threshold=0.3)\n",
    "\n",
    "model.to(device)\n",
    "model_lesion.to(device)\n",
    "\n",
    "model.eval()\n",
    "model_lesion.eval()\n",
    "print(\"Done loading models\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Folderele de lucru"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "load_path = 'G:/MachineLearning/DICOM_Database/demo/venos/'\n",
    "out_path = 'G:/MachineLearning/DICOM_Database/demo/venos segmentat/'\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)\n",
    "\n",
    "dicom_files = glob.glob(load_path + '*')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ceva parametri de lucru"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "window_min = -250.0\n",
    "window_max = 250.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Iteram pe rand prin fiecare imagine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/57 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ef168fe939b464fa482745511a6b312"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_3048/1694092032.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     33\u001B[0m     \u001B[1;31m# Extragem ficatul pe baza mastii\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     34\u001B[0m     \u001B[0mprops\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mregionprops\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmask_liver\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mint32\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 35\u001B[1;33m     \u001B[1;33m(\u001B[0m\u001B[0mminz\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mminx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mminy\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmaxz\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmaxx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmaxy\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprops\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbbox\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     36\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     37\u001B[0m     \u001B[0mcrop_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmaxx\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mminx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmaxy\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mminy\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;36m5\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for dicom_path in tqdm(dicom_files):\n",
    "    datastore = dcmread(dicom_path)\n",
    "    image = datastore.pixel_array\n",
    "    image = apply_modality_lut(image, datastore)\n",
    "    image = image.astype(np.float32)\n",
    "\n",
    "\n",
    "    # Procesarea initiala a imaginii DICOM\n",
    "    image[image < window_min] = window_min\n",
    "    image[image > window_max] = window_max\n",
    "    image = (image - window_min) / (window_max - window_min)\n",
    "\n",
    "    # Chambolle TV denoising\n",
    "    image = denoise_tv_chambolle(image, weight=0.02)\n",
    "\n",
    "    init_image = torch.tensor(image)\n",
    "    init_size = init_image.shape[0]\n",
    "\n",
    "    resize_trans = transforms.Compose([\n",
    "        transforms.Resize(init_size, transforms.InterpolationMode.BILINEAR, antialias=True)\n",
    "    ])\n",
    "\n",
    "    # Generam segmentarea ficatului\n",
    "    input_tensor = transform_ct(image)\n",
    "    input_batch = input_tensor.repeat(2, 1, 1, 1)\n",
    "    input_batch = input_batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)[0]\n",
    "\n",
    "    output = resize_trans(output)\n",
    "    mask_liver = torch.round(output)\n",
    "\n",
    "    # Extragem ficatul pe baza mastii\n",
    "    props = regionprops(mask_liver.numpy().astype(np.int32))\n",
    "    (minz, minx, miny, maxz, maxx, maxy) = props[0].bbox\n",
    "\n",
    "    crop_size = max(maxx - minx, maxy - miny) + 5\n",
    "    crop_center = (int((maxx + minx) / 2), int((maxy + miny) / 2))\n",
    "\n",
    "    bounds = [max(crop_center[0] - int(crop_size / 2), 0),\n",
    "              min(crop_center[0] + int(crop_size / 2), image.shape[0]),\n",
    "              max(crop_center[1] - int(crop_size / 2), 0),\n",
    "              min(crop_center[1] + int(crop_size / 2), image.shape[1])]\n",
    "\n",
    "    only_liver = np.copy(image)\n",
    "    only_liver[mask_liver.detach().numpy().squeeze() == 0] = 0\n",
    "\n",
    "    liver_image = np.copy(only_liver[bounds[0]:bounds[1], bounds[2]:bounds[3]]) # Taiem doar ce ne intereseaza\n",
    "\n",
    "    zoom_liver_mask = np.copy(liver_image)\n",
    "    zoom_liver_mask[zoom_liver_mask > 0] = 1\n",
    "\n",
    "    if np.max(liver_image.flatten()) > 0.001:\n",
    "        mean = np.mean(liver_image[liver_image > 0.0001].flatten())\n",
    "        liver_image[liver_image < 0.0001] = mean\n",
    "        liver_image = exposure.equalize_adapthist(liver_image, clip_limit=0.01)\n",
    "\n",
    "    restore_size = transforms.Resize(liver_image.shape, interpolation=transforms.functional.InterpolationMode.BILINEAR, antialias=True)\n",
    "\n",
    "    liver_tensor = transform_liver(liver_image)\n",
    "    zoom_liver_mask = transform_liver(zoom_liver_mask)\n",
    "\n",
    "    input_batch = liver_tensor.repeat(2, 1, 1, 1)\n",
    "    input_batch = input_batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model_lesion(input_batch)[0]\n",
    "\n",
    "    output[zoom_liver_mask < 1] = 0\n",
    "    output = restore_size(output)\n",
    "    output[output > 0.05] = 1\n",
    "    output[output < 0.05] = 0\n",
    "\n",
    "    mask_lesion = np.zeros((init_size, init_size))\n",
    "    mask_lesion[bounds[0]:bounds[1], bounds[2]:bounds[3]] = output.detach().numpy().squeeze()\n",
    "    for i in range(5):\n",
    "        mask_lesion = dilation(mask_lesion)\n",
    "\n",
    "    mask_lesion = torch.tensor(mask_lesion)\n",
    "\n",
    "    seg_result = torchvision.utils.draw_segmentation_masks((init_image*255).repeat(3, 1, 1).type(torch.uint8),\n",
    "                                                    (mask_liver*255).type(torch.bool),\n",
    "                                                    0.3, \"green\")\n",
    "\n",
    "    seg_result = torchvision.utils.draw_segmentation_masks(seg_result,\n",
    "                                                           (mask_lesion*255).type(torch.bool),\n",
    "                                                           0.4, \"red\")\n",
    "\n",
    "    # Magie cu tag-uri DICOM ca sa arate corect culorile\n",
    "    datastore.PhotometricInterpretation = 'RGB'\n",
    "    datastore.SamplesPerPixel = 3\n",
    "    datastore.BitsAllocated = 8\n",
    "    datastore.BitsStored = 8\n",
    "    datastore.HighBit = 7\n",
    "\n",
    "    datastore.add_new(0x00280006, 'US', 0)\n",
    "    datastore.is_little_endian = True\n",
    "\n",
    "    datastore.PixelData = np.flipud(np.moveaxis(seg_result.cpu().numpy(), 0, 2)).tobytes()\n",
    "\n",
    "    # In final salvam fisierul DICOM cu masca\n",
    "    save_path = out_path + os.path.basename(dicom_path)\n",
    "    datastore.fix_meta_info()\n",
    "    datastore.save_as(save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(mask_lesion.detach().numpy().squeeze(), cmap='gray')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(liver_image.squeeze(), cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}